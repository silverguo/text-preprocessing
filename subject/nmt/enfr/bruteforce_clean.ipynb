{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# load text from file\n",
    "def load_text(file_path, encoding='UTF-8'):\n",
    "    \n",
    "    docs = []\n",
    "    with open(file_path, 'r', encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            docs.append(line.strip())\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get language code by fasttext\n",
    "def get_lang(docs, model_path, kmost=1):\n",
    "\n",
    "    lang_classifier = fasttext.load_model(model_path)\n",
    "\n",
    "    return lang_classifier.predict(docs, kmost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# save text to file\n",
    "def save_text(text, file_path, encoding='UTF-8'):\n",
    "    # write to output\n",
    "    with open(file_path, 'w', encoding=encoding) as f:\n",
    "        f.write('\\n'.join(text))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# remove duplicate pair\n",
    "def pair_deduplicate(doc_left, doc_right):\n",
    "    \n",
    "    doc_left_clean = []\n",
    "    doc_right_clean = []\n",
    "    \n",
    "    # already exist\n",
    "    doc_contain = set()\n",
    "    \n",
    "    for dl, dr in zip(doc_left, doc_right):\n",
    "        if (dl not in doc_contain) and (dr not in doc_contain):\n",
    "            doc_left_clean.append(dl)\n",
    "            doc_right_clean.append(dr)\n",
    "        \n",
    "        doc_contain.add(dl)\n",
    "        doc_contain.add(dr)\n",
    "        \n",
    "    print('remove duplicate from langauge pair')\n",
    "    print('from {0} to {1}'.format(len(doc_left), len(doc_left_clean)))\n",
    "    \n",
    "    return doc_left_clean, doc_right_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# remove oversize pair\n",
    "def pair_oversize(doc_left, doc_right, th_num=120):\n",
    "    \n",
    "    doc_left_clean = []\n",
    "    doc_right_clean = []\n",
    "    \n",
    "    for dl, dr in zip(doc_left, doc_right):\n",
    "        \n",
    "        ll = len(dl.split())\n",
    "        lr = len(dr.split())\n",
    "        \n",
    "        if (ll < th_num) and (lr < th_num):\n",
    "            doc_left_clean.append(dl)\n",
    "            doc_right_clean.append(dr)\n",
    "        \n",
    "    print('remove oversize pair from langauge pair')\n",
    "    print('from {0} to {1}'.format(len(doc_left), len(doc_left_clean)))\n",
    "    \n",
    "    return doc_left_clean, doc_right_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# remove mislength pair\n",
    "def pair_mislength(doc_left, doc_right, ratio=1.8, verbose=False):\n",
    "    \n",
    "    doc_left_clean = []\n",
    "    doc_right_clean = []\n",
    "    \n",
    "    doc_verbose = []\n",
    "    \n",
    "    for dl, dr in zip(doc_left, doc_right):\n",
    "        \n",
    "        ll = len(dl.split())\n",
    "        lr = len(dr.split())\n",
    "        \n",
    "        if (ll < lr*ratio) and (lr < ll*ratio):\n",
    "            doc_left_clean.append(dl)\n",
    "            doc_right_clean.append(dr)\n",
    "        elif (ll < 8 or lr < 8) and (abs(ll-lr) < 8):\n",
    "            doc_left_clean.append(dl)\n",
    "            doc_right_clean.append(dr)\n",
    "        elif verbose:\n",
    "            doc_verbose.append(dl+' || '+dr)\n",
    "        \n",
    "    print('remove mislength from langauge pair')\n",
    "    print('from {0} to {1}'.format(len(doc_left), len(doc_left_clean)))\n",
    "    \n",
    "    if verbose:\n",
    "        print('removed mislength sentences')\n",
    "        for idx in random.sample(range(0,len(doc_verbose)), 25):\n",
    "            print(doc_verbose[idx])\n",
    "    \n",
    "    return doc_left_clean, doc_right_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# remove mislang pair\n",
    "def pair_mislang(doc_left, doc_right, lang_left, lang_right, model_path, kmost=2, verbose=False):\n",
    "    \n",
    "    res_left = get_lang(doc_left, model_path, kmost)\n",
    "    res_right = get_lang(doc_right, model_path, kmost)\n",
    "    \n",
    "    doc_left_clean = []\n",
    "    doc_right_clean = []\n",
    "    \n",
    "    doc_verbose = []\n",
    "    \n",
    "    for idx in range(len(doc_left)):\n",
    "        \n",
    "        rl = [l.split('__')[-1] for l in res_left[0][idx]]\n",
    "        rr = [l.split('__')[-1] for l in res_right[0][idx]]\n",
    "        \n",
    "        if (lang_left in rl) and (lang_right in rr):\n",
    "            doc_left_clean.append(doc_left[idx])\n",
    "            doc_right_clean.append(doc_right[idx])\n",
    "        elif verbose:\n",
    "            doc_verbose.append(doc_left[idx]+' || '+doc_right[idx])\n",
    "    \n",
    "    print('remove mislength from langauge pair')\n",
    "    print('from {0} to {1}'.format(len(doc_left), len(doc_left_clean)))\n",
    "    \n",
    "    if verbose:\n",
    "        print('removed mislength sentences')\n",
    "        for idx in random.sample(range(0,len(doc_verbose)), 25):\n",
    "            print(doc_verbose[idx])\n",
    "    \n",
    "    return doc_left_clean, doc_right_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_en = load_text('./data/raw/bruteforce/raw-enfr.en')\n",
    "all_fr = load_text('./data/raw/bruteforce/raw-enfr.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove duplicate from langauge pair\n",
      "from 41731691 to 33712468\n"
     ]
    }
   ],
   "source": [
    "all_en, all_fr = pair_deduplicate(all_en, all_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove oversize pair from langauge pair\n",
      "from 33712468 to 33424814\n"
     ]
    }
   ],
   "source": [
    "all_en, all_fr = pair_oversize(all_en, all_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove mislength from langauge pair\n",
      "from 33424814 to 28577869\n"
     ]
    }
   ],
   "source": [
    "all_en, all_fr = pair_mislength(all_en, all_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "fastText: Cannot load ./model/lid.176.bin due to C++ extension failed to allocate the memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32mfasttext/fasttext.pyx\u001b[0m in \u001b[0;36mfasttext.fasttext.load_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: vector::_M_default_append",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7a3b2497e9a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_fr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair_mislang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_fr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./model/lid.176.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-036b02d138a6>\u001b[0m in \u001b[0;36mpair_mislang\u001b[0;34m(doc_left, doc_right, lang_left, lang_right, model_path, kmost, verbose)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpair_mislang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mres_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mres_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-0d7b8f0f6721>\u001b[0m in \u001b[0;36mget_lang\u001b[0;34m(docs, model_path, kmost)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlang_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlang_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfasttext/fasttext.pyx\u001b[0m in \u001b[0;36mfasttext.fasttext.load_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: fastText: Cannot load ./model/lid.176.bin due to C++ extension failed to allocate the memory"
     ]
    }
   ],
   "source": [
    "all_en, all_fr = pair_mislang(all_en, all_fr, 'en', 'fr', './model/lid.176.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am grateful to all Members who contributed their ideas, and I am startled by the degree of consensus that exists. || J'exprime ma reconnaissance à tous les membres qui y ont collaboré par leurs idées, et je suis surpris par le niveau de consensus existant.\n",
      "There is an appetite in this Chamber to work fast, to work together, in search of a lasting solution to Europe's energy crisis, and we must harness that. || Il y a dans cette Assemblée un goût pour le travail rapide, le travail en commun à la recherche d'une solution durable à la crise énergétique de l'Europe, et nous devons l'exploiter.\n",
      "Of all the potential plans to open up a new energy era, one stands out: it is called the supergrid, or DESERTEC. || Parmi tous les plans susceptibles d'ouvrir une nouvelle ère énergétique, il en est un qui se démarque des autres: il est appelé le super-réseau, ou DESERTEC.\n"
     ]
    }
   ],
   "source": [
    "win = 3\n",
    "sid = random.randint(0, len(europarl_en_clean)-win-1)\n",
    "for idx in range(sid, sid+win):\n",
    "    print(europarl_en_clean[idx], '||', europarl_fr_clean[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_text(europarl_en_clean, './data/clean/europarl-enfr-clean.en')\n",
    "save_text(europarl_fr_clean, './data/clean/europarl-enfr-clean.fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# tatoeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfBase = pd.read_csv('./data/raw/tatoeba/sentences.csv', sep='\\t', header=None, names=['label', 'lang', 'text'])\n",
    "dfBase = dfBase.set_index('label')\n",
    "dfLink = pd.read_csv('./data/raw/tatoeba/links.csv', sep='\\t', header=None, names=['ida', 'idb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     5
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# en fr sentences label\n",
    "label_en = set(dfBase[dfBase['lang']=='eng'].index.tolist())\n",
    "label_fr = set(dfBase[dfBase['lang']=='fra'].index.tolist())\n",
    "lang_pair = set()\n",
    "# get lang pair label\n",
    "for r in dfLink.itertuples():\n",
    "    if r[1] in label_en and r[2] in label_fr:\n",
    "        lang_pair.add((r[1], r[2]))\n",
    "    elif r[2] in label_en and r[1] in label_fr:\n",
    "        lang_pair.add((r[2], r[1]))\n",
    "lang_enfr = list(lang_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tatoeba has 231165 sentences pair for english and french\n"
     ]
    }
   ],
   "source": [
    "# extract lang pair\n",
    "tatoeba_en = []\n",
    "tatoeba_fr = []\n",
    "for lpair in lang_enfr:\n",
    "    tatoeba_en.append(dfBase.loc[lpair[0],'text'].strip())\n",
    "    tatoeba_fr.append(dfBase.loc[lpair[1],'text'].strip())\n",
    "print('tatoeba has {} sentences pair for english and french'.format(len(tatoeba_en)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not talking to you. || Je ne parle pas avec toi.\n",
      "I just want you to have it. || Je veux juste que tu l'aies.\n",
      "The godmother baked a delicious cake. || La marraine cuisit un délicieux gâteau.\n"
     ]
    }
   ],
   "source": [
    "win = 3\n",
    "sid = random.randint(0, len(tatoeba_en)-win-1)\n",
    "for idx in range(sid, sid+win):\n",
    "    print(tatoeba_en[idx], '||', tatoeba_fr[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_text(tatoeba_en, './data/clean/tatoeba-enfr-clean.en')\n",
    "save_text(tatoeba_fr, './data/clean/tatoeba-enfr-clean.fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# jrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('./data/raw/jrc/alignedCorpus-en-fr.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "jrc_en = []\n",
    "jrc_fr = []\n",
    "\n",
    "# extract and clean\n",
    "anti_dup = set()\n",
    "for t in root.findall('.//link'):\n",
    "    \n",
    "    # get text\n",
    "    \n",
    "    # s1 or s2 may contain p\n",
    "    if t.find('s1').find('p') is None:\n",
    "        sent_en = t.find('s1').text\n",
    "    else:\n",
    "        sent_en = ' '.join([p.text for p in t.find('s1').findall('p')])\n",
    "    if t.find('s2').find('p') is None:\n",
    "        sent_fr = t.find('s2').text\n",
    "    else:\n",
    "        sent_fr = ' '.join([p.text for p in t.find('s2').findall('p')])\n",
    "        \n",
    "    # clean\n",
    "    \n",
    "    # start or end space\n",
    "    sent_en = sent_en.strip()\n",
    "    sent_fr = sent_fr.strip()\n",
    "    \n",
    "    # useless line\n",
    "    if sent_en.startswith('Article ') or sent_fr.startswith('Article '):\n",
    "        continue\n",
    "    if ('%gt%' in sent_en) or ('%gt%' in sent_fr):\n",
    "        continue\n",
    "    if ('http://' in sent_en) or ('http://' in sent_fr):\n",
    "        continue\n",
    "    if ('https://' in sent_en) or ('https://' in sent_fr):\n",
    "        continue\n",
    "    if sent_en.startswith('[1]') or sent_fr.startswith('[1]'):\n",
    "        continue\n",
    "    if (sent_en.startswith('(') and sent_en.endswith(')')):\n",
    "        continue\n",
    "    if (sent_fr.startswith('(') and sent_fr.endswith(')')):\n",
    "        continue\n",
    "    if len(sent_en.split('|')) > 2:\n",
    "        continue\n",
    "    if len(sent_fr.split('|')) > 2:\n",
    "        continue\n",
    "    if len(sent_en.split('/')) > 2:\n",
    "        continue\n",
    "    if len(sent_en.split('/')) > 2:\n",
    "        continue\n",
    "    \n",
    "    # mismatch upper percentage\n",
    "    upper_en = sum([1 for c in ''.join(sent_en.split()) if c.isupper()]) * 1.0 / len(''.join(sent_en.split()))\n",
    "    upper_fr = sum([1 for c in ''.join(sent_fr.split()) if c.isupper()]) * 1.0 / len(''.join(sent_fr.split()))\n",
    "    \n",
    "    if abs(upper_en - upper_fr) > 0.5:\n",
    "        continue\n",
    "    \n",
    "    # sentence index\n",
    "    idx_flag = False\n",
    "    for idx in range(1, 100):\n",
    "        pfa = str(idx)+'. '\n",
    "        pfb = str(idx)+' . '\n",
    "        enj = sent_en.startswith(pfa) or sent_en.startswith(pfb)\n",
    "        frj = sent_fr.startswith(pfa) or sent_fr.startswith(pfb)\n",
    "        \n",
    "        if enj and frj:\n",
    "            sent_en = sent_en.split('.', 1)[1].strip()\n",
    "            sent_fr = sent_fr.split('.', 1)[1].strip()\n",
    "            break\n",
    "        \n",
    "        if enj != frj:\n",
    "            idx_flag = True\n",
    "            break\n",
    "    if idx_flag:\n",
    "        continue\n",
    "    \n",
    "    # sentence index\n",
    "    idx_flag = False\n",
    "    for idx in range(1, 200):\n",
    "        pfa = str(idx)+'. '\n",
    "        pfb = str(idx)+'. '\n",
    "        enj = sent_en.startswith(pfa) or sent_en.startswith(pfb)\n",
    "        frj = sent_fr.startswith(pfa) or sent_fr.startswith(pfb)\n",
    "        \n",
    "        if enj and frj:\n",
    "            sent_en = sent_en.split('.', 1)[1].strip()\n",
    "            sent_fr = sent_fr.split('.', 1)[1].strip()\n",
    "            break\n",
    "        \n",
    "        if enj != frj:\n",
    "            idx_flag = True\n",
    "            break\n",
    "    if idx_flag:\n",
    "        continue\n",
    "    \n",
    "    idx_flag = False\n",
    "    for idx in range(1, 200):\n",
    "        \n",
    "        pfa = '('+str(idx)+')'\n",
    "        pfb = str(idx)+')'\n",
    "        enj = sent_en.startswith(pfa) or sent_en.startswith(pfb)\n",
    "        frj = sent_fr.startswith(pfa) or sent_fr.startswith(pfb)\n",
    "        \n",
    "        if enj and frj:\n",
    "            sent_en = sent_en.split(')', 1)[1].strip()\n",
    "            sent_fr = sent_fr.split(')', 1)[1].strip()\n",
    "            break\n",
    "        \n",
    "        if enj != frj:\n",
    "            idx_flag = True\n",
    "            break\n",
    "    if idx_flag:\n",
    "        continue\n",
    "    \n",
    "    idx_flag = False\n",
    "    for lidx in ['a', 'b', 'c', 'd', 'e', 'f', 'g',  \n",
    "                 'A', 'B', 'C', 'D', 'E', 'F', 'G']:\n",
    "        \n",
    "        pfa = '('+lidx+')'\n",
    "        pfb = lidx+')'\n",
    "        enj = sent_en.startswith(pfa) or sent_en.startswith(pfb)\n",
    "        frj = sent_fr.startswith(pfa) or sent_fr.startswith(pfb)\n",
    "        \n",
    "        if enj and frj:\n",
    "            sent_en = sent_en.split(')', 1)[1].strip()\n",
    "            sent_fr = sent_fr.split(')', 1)[1].strip()\n",
    "            break\n",
    "        \n",
    "        if enj != frj:\n",
    "            idx_flag = True\n",
    "            break\n",
    "    if idx_flag:\n",
    "        continue\n",
    "    \n",
    "    idx_flag = False\n",
    "    for idx in range(1, 4):\n",
    "        \n",
    "        pfa = '('+'i'*idx+')'\n",
    "        pfb = 'i'*idx+')'\n",
    "        enj = sent_en.startswith(pfa) or sent_en.startswith(pfb)\n",
    "        frj = sent_fr.startswith(pfa) or sent_fr.startswith(pfb)\n",
    "        \n",
    "        if enj and frj:\n",
    "            sent_en = sent_en.split(')', 1)[1].strip()\n",
    "            sent_fr = sent_fr.split(')', 1)[1].strip()\n",
    "            break\n",
    "        \n",
    "        if enj != frj:\n",
    "            idx_flag = True\n",
    "            break\n",
    "    if idx_flag:\n",
    "        continue\n",
    "    \n",
    "    idx_flag = False\n",
    "    for lidx in ['- ']:\n",
    "        \n",
    "        enj = sent_en.startswith(lidx)\n",
    "        frj = sent_fr.startswith(lidx)\n",
    "        \n",
    "        if enj and frj:\n",
    "            sent_en = sent_en[len(lidx):]\n",
    "            sent_fr = sent_fr[len(lidx):]\n",
    "            break\n",
    "        \n",
    "        if enj != frj:\n",
    "            idx_flag = True\n",
    "            break\n",
    "    if idx_flag:\n",
    "        continue\n",
    "    \n",
    "    # replace special\n",
    "    sent_en = sent_en.replace('%quot%', '\"')\n",
    "    sent_fr = sent_fr.replace('%quot%', '\"')\n",
    "    \n",
    "    sent_en = sent_en.replace('º', 'o')\n",
    "    sent_fr = sent_fr.replace('º', 'o')\n",
    "    \n",
    "    \n",
    "    # append\n",
    "    if (sent_en not in anti_dup) and (sent_fr not in anti_dup):\n",
    "        anti_dup.add(sent_en)\n",
    "        anti_dup.add(sent_fr)\n",
    "        \n",
    "        jrc_en.append(sent_en)\n",
    "        jrc_fr.append(sent_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove mislength from langauge pair\n",
      "from 579601 to 576223\n",
      "remove mislength from langauge pair\n",
      "from 576223 to 523722\n"
     ]
    }
   ],
   "source": [
    "temp_en, temp_fr = pair_mislength(jrc_en, jrc_fr)\n",
    "temp_en, temp_fr = pair_mislang(temp_en, temp_fr, 'en', 'fr', './model/lid.176.bin', 1)\n",
    "jrc_en_clean, jrc_fr_clean = temp_en, temp_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.31. The Commission is of the opinion that it would not be appropriate in the AAR to distinguish, as the Court suggests, between expenditure which has already been verified and expenditure which will be subject to further verification. Given the multi-annual nature of the conformity clearance process, such a distinction would be largely arbitrary; it would also prejudice the Member States' right of defence in the context of the clearance of accounts procedure. || 4.31. Contrairement à ce que suggère la Cour, la Commission considère comme inapproprié le fait d'établir une distinction dans le RAA entre les dépenses déjà contrôlées et celles faisant l'objet de vérifications supplémentaires. Compte tenu du caractère pluriannuel de la procédure d'apurement de conformité, une telle distinction serait en grande partie arbitraire et porterait également atteinte au droit de réponse des États membres dans le cadre de la procédure d'apurement des comptes.\n",
      "The Director-General’s declaration and the accompanying AAR need to be placed in the broader regulatory framework for the clearance of the EAGGF accounts set out in Article 7 of Regulation (EC) No 1258/1999. The whole process is divided into an annual financial clearance on the one hand and a multi-annual conformity clearance on the other, which allows the Commission to exclude expenditure from Community financing that has been effected less than 24 months prior to the Commission’s written communication of the results of its audits to the Member State concerned. || La déclaration du directeur général et le RAA qui l'accompagne doivent être envisagés dans le contexte règlementaire plus large prévu pour l'apurement des comptes FEOGA à l'article 7 du règlement (CE) no 1258/1999. L'ensemble de la procédure comprend, d'une part, un apurement financier annuel et, d'autre part, un apurement de conformité pluriannuel, ce qui permet à la Commission d'exclure du financement communautaire les dépenses encourues moins de vingt-quatre mois avant que la Commission ne communique les résultats de ses audits à l'État membre concerné.\n",
      "Against this background, reasonable assurance as regards the legality and regularity of agricultural expenditure carried out under shared management cannot be obtained on an annual basis, in particular in the context of the Director-General’s AAR, from the audit work of the Commission services alone, but must be predicated on the premise that Member States fulfil their management and control obligations. Where Commission audits subsequently identify deficiencies, the risk of ineligible expenditure will be determined and appropriate financial corrections applied. || Dans ce contexte, les travaux d'audit de la Commission ne permettent pas à eux seuls d'obtenir, sur une base annuelle, notamment dans le cadre du RAA du directeur général, une assurance raisonnable quant à la légalité et à la régularité des dépenses agricoles effectuées dans le cadre de la gestion partagée; il est donc nécessaire de vérifier sur place que les États membres respectent leurs obligations en matière de gestion et de contrôle. Si les audits de la Commission révèlent ensuite des lacunes, on déterminera le risque de dépenses inéligibles et des corrections financières appropriées seront appliquées.\n"
     ]
    }
   ],
   "source": [
    "win = 3\n",
    "sid = random.randint(0, len(jrc_en_clean)-win-1)\n",
    "for idx in range(sid, sid+win):\n",
    "    print(jrc_en_clean[idx], '||', jrc_fr_clean[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_text(jrc_en_clean, './data/clean/jrc-enfr-clean.en')\n",
    "save_text(jrc_fr_clean, './data/clean/jrc-enfr-clean.fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# giga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "giga_en = load_text('./data/raw/giga/giga-fren.release2.fixed.en')\n",
    "giga_fr = load_text('./data/raw/giga/giga-fren.release2.fixed.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove duplicate from langauge pair\n",
      "from 22520376 to 19630451\n",
      "remove mislength from langauge pair\n",
      "from 19630451 to 17333151\n",
      "remove mislength from langauge pair\n",
      "from 17333151 to 16722019\n"
     ]
    }
   ],
   "source": [
    "temp_en, temp_fr = pair_deduplicate(giga_en, giga_fr)\n",
    "temp_en, temp_fr = pair_mislength(temp_en, temp_fr, ratio=1.5)\n",
    "temp_en, temp_fr = pair_mislang(temp_en, temp_fr, 'en', 'fr', './model/lid.176.bin', 1)\n",
    "giga_en_clean, giga_fr_clean = temp_en, temp_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional details on submitting comments and the Government’s consultation process can be found on the Department of Finance Web site at www.fin.gc.ca. Finally, as committed to in the Government’s response to the Standing Committee on Human Resources Development and the Status of Persons with Disabilities, in 2003 the Department of Finance will initiate an evaluation of the DTC to ensure that it achieves its policy intent of providing tax assistance to persons with a severe and prolonged mental or physical impairment, the effects of which markedly restrict the ability to perform a basic activity of daily living. || On peut trouver d’autres précisions sur la présentation de commentaires et sur le processus de consultation du gouvernement sur le site Web du ministère des Finances à l’adresse www.fin.gc.ca. Enfin, conformément à la réponse du gouvernement au Comité permanent du développement des ressources humaines et de la condition des personnes handicapées, le ministère des Finances entreprendra en 2003 une évaluation du CIPH pour veiller à ce que celui-ci respecte son intention de principe de fournir une aide fiscale aux personnes qui ont une déficience intellectuelle ou physique grave et prolongée dont les effets limitent de façon marquée leur capacité d’exécuter des activités courantes de la vie quotidienne.\n",
      "In all of its activities on matters affecting persons with disabilities, the Government continues to be guided by compassion and inclusion, ensuring a society in which spending programs and tax initiatives respond to the needs of persons with disabilities and those who care for them. || Dans le cadre de toutes ses activités touchant les personnes handicapées, le gouvernement continuera d’être orienté par la compassion et l’inclusion sociale, assurant ainsi une société dans laquelle les programmes de dépenses et les initiatives fiscales satisfont aux besoins des personnes handicapées et des personnes qui s’en occupent.\n",
      "Annex 1 The Government of Canada provides substantial support to persons with disabilities through a number of tax measures and direct spending programs. || Annexe 1 Le gouvernement du Canada fournit un appui important aux personnes handicapées au moyen d’un certain nombre de mesures fiscales et de programmes de dépenses directes.\n"
     ]
    }
   ],
   "source": [
    "win = 3\n",
    "sid = random.randint(0, len(giga_en_clean)-win-1)\n",
    "for idx in range(sid, sid+win):\n",
    "    print(giga_en_clean[idx], '||', giga_fr_clean[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_text(giga_en_clean, './data/clean/giga-enfr-clean.en')\n",
    "save_text(giga_fr_clean, './data/clean/giga-enfr-clean.fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "un_en = load_text('./data/raw/un/undoc.2000.fr-en.en')\n",
    "un_fr = load_text('./data/raw/un/undoc.2000.fr-en.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove duplicate from langauge pair\n",
      "from 12886831 to 9313528\n",
      "remove mislength from langauge pair\n",
      "from 9313528 to 8658767\n",
      "remove mislength from langauge pair\n",
      "from 8658767 to 8481341\n"
     ]
    }
   ],
   "source": [
    "temp_en, temp_fr = pair_deduplicate(un_en, un_fr)\n",
    "temp_en, temp_fr = pair_mislength(temp_en, temp_fr, ratio=1.5)\n",
    "temp_en, temp_fr = pair_mislang(temp_en, temp_fr, 'en', 'fr', './model/lid.176.bin', 1)\n",
    "un_en_clean, un_fr_clean = temp_en, temp_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One or more creditors that are owed a matured debt, in which case the creditor(s) should show that the debt has matured and is unpaid.” || Un ou plusieurs créanciers détenant une créance échue, auquel cas il(s) devrai(en)t démontrer que cette créance est échue et impayée.”\n",
      "It was noted that that proposal was intended to establish minimum agreed entry criteria, and that the draft Guide could note and discuss potential variations, such as a requirement for a minimum amount of debt or that the debt need not be mature. || Il a été noté que cette proposition visait à établir des critères d'admissibilité convenus minimaux et que le projet de guide pourrait indiquer et examiner des variantes possibles, par exemple qu'il faut un montant minimum de créances ou qu'il n'est pas nécessaire que la créance soit échue.\n",
      "While there was agreement for taking that general approach, some support was expressed in favour of the test being that the debtor “is unable or will be unable to pay its debts as and when they fall due.” || Malgré un accord en faveur de l'adoption de cette approche générale, le critère énonçant que le débiteur “est ou sera dans l'incapacité de payer ses dettes à l'échéance” a bénéficié d'un certain appui.\n"
     ]
    }
   ],
   "source": [
    "win = 3\n",
    "sid = random.randint(0, len(un_en_clean)-win-1)\n",
    "for idx in range(sid, sid+win):\n",
    "    print(un_en_clean[idx], '||', un_fr_clean[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_text(un_en_clean, './data/clean/un-enfr-clean.en')\n",
    "save_text(un_fr_clean, './data/clean/un-enfr-clean.fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_en = []\n",
    "all_fr = []\n",
    "for pref_path in list(set([s.rsplit('.', 1)[0] for s in glob.glob('./data/clean/*')])):\n",
    "    all_en += load_text(pref_path+'.en')\n",
    "    all_fr += load_text(pref_path+'.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without appropriate policy frameworks to address the sector issues, and without having a national energy strategy for Canada, regulations appear to be headed for even greater complexities. || Sans les politiques cadres permettant de s’attaquer aux enjeux du secteur et sans une stratégie énergétique nationale, la réglementation semble devoir se complexifier encore davantage.\n",
      "• Access to capital, and private sector investment in the energy sector, is also considered an issue by the industry. || • L’industrie considère également que l’accès au capital ainsi que l’investissement privé dans le secteur énergétique sont un enjeu.\n",
      "• There is a continuing need for government funding for R&D in the energy sector, especially to address the need for developing and adopting SD tools, practices and technologies applicable to the sector. || • Ce secteur a continuellement besoin de fonds gouvernementaux pour sa R­D, particulièrement pour répondre aux besoins d’élaborer et d’adopter des outils, pratiques et technologies de DD adéquats.\n"
     ]
    }
   ],
   "source": [
    "win = 3\n",
    "sid = random.randint(0, len(all_en)-win-1)\n",
    "for idx in range(sid, sid+win):\n",
    "    print(all_en[idx], '||', all_fr[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove duplicate from langauge pair\n",
      "from 27884964 to 27741249\n",
      "remove oversize pair from langauge pair\n",
      "from 27741249 to 26922456\n"
     ]
    }
   ],
   "source": [
    "temp_en, temp_fr = pair_deduplicate(all_en, all_fr)\n",
    "all_en_clean, all_fr_clean = pair_oversize(temp_en, temp_fr, th_num=80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

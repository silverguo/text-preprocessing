{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# load text from file\n",
    "def load_text(file_path, encoding='UTF-8'):\n",
    "    \n",
    "    docs = []\n",
    "    with open(file_path, 'r', encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            docs.append(line.strip())\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# get language code by fasttext\n",
    "def get_lang(docs, model_path, kmost=1):\n",
    "\n",
    "    lang_classifier = fastText.load_model(model_path)\n",
    "\n",
    "    return lang_classifier.predict(docs, kmost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# save text to file\n",
    "def save_text(text, file_path, encoding='UTF-8'):\n",
    "    # write to output\n",
    "    with open(file_path, 'w', encoding=encoding) as f:\n",
    "        f.write('\\n'.join(text))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# remove duplicate pair\n",
    "def pair_deduplicate(doc_left, doc_right):\n",
    "    \n",
    "    doc_left_clean = []\n",
    "    doc_right_clean = []\n",
    "    \n",
    "    # already exist\n",
    "    doc_contain = set()\n",
    "    \n",
    "    for dl, dr in zip(doc_left, doc_right):\n",
    "        if (dl not in doc_contain) and (dr not in doc_contain):\n",
    "            doc_left_clean.append(dl)\n",
    "            doc_right_clean.append(dr)\n",
    "        \n",
    "        doc_contain.add(dl)\n",
    "        doc_contain.add(dr)\n",
    "        \n",
    "    print('remove duplicate from langauge pair')\n",
    "    print('from {0} to {1}'.format(len(doc_left), len(doc_left_clean)))\n",
    "    \n",
    "    return doc_left_clean, doc_right_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# remove mislength pair\n",
    "def pair_mislength(doc_left, doc_right, ratio=1.8):\n",
    "    \n",
    "    doc_left_clean = []\n",
    "    doc_right_clean = []\n",
    "    \n",
    "    for dl, dr in zip(doc_left, doc_right):\n",
    "        \n",
    "        ll = len(dl.split())\n",
    "        lr = len(dr.split())\n",
    "        \n",
    "        if (ll < lr*ratio) and (lr < ll*ratio):\n",
    "            doc_left_clean.append(dl)\n",
    "            doc_right_clean.append(dr)\n",
    "        elif (ll < 8 or lr < 8) and (abs(ll-lr) < 8):\n",
    "            doc_left_clean.append(dl)\n",
    "            doc_right_clean.append(dr)\n",
    "        \n",
    "    print('remove mislength from langauge pair')\n",
    "    print('from {0} to {1}'.format(len(doc_left), len(doc_left_clean)))\n",
    "    \n",
    "    return doc_left_clean, doc_right_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# remove mislang pair\n",
    "def pair_mislang(doc_left, doc_right, lang_left, lang_right, model_path, kmost=2):\n",
    "    \n",
    "    res_left = get_lang(doc_left, model_path, kmost)\n",
    "    res_right = get_lang(doc_right, model_path, kmost)\n",
    "    \n",
    "    doc_left_clean = []\n",
    "    doc_right_clean = []\n",
    "    \n",
    "    for idx in range(len(doc_left)):\n",
    "        \n",
    "        rl = [l.split('__')[-1] for l in res_left[0][idx]]\n",
    "        rr = [l.split('__')[-1] for l in res_right[0][idx]]\n",
    "        \n",
    "        if (lang_left in rl) and (lang_right in rr):\n",
    "            doc_left_clean.append(doc_left[idx])\n",
    "            doc_right_clean.append(doc_right[idx])\n",
    "    \n",
    "    print('remove mislength from langauge pair')\n",
    "    print('from {0} to {1}'.format(len(doc_left), len(doc_left_clean)))\n",
    "    \n",
    "    return doc_left_clean, doc_right_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# europarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "europarl_en = load_text('./data/raw/europarl/europarl-v7.fr-en.en')\n",
    "europarl_fr = load_text('./data/raw/europarl/europarl-v7.fr-en.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_en, temp_fr = pair_deduplicate(europarl_en, europarl_fr)\n",
    "temp_en, temp_fr = pair_mislength(temp_en, temp_fr)\n",
    "temp_en, temp_fr = pair_mislang(temp_en, temp_fr, 'en', 'fr', './model/lid.176.bin', 2)\n",
    "europarl_en_clean, europarl_fr_clean = temp_en, temp_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sid, win = 45000, 3\n",
    "for idx in range(sid, sid+win):\n",
    "    print(europarl_en_clean[idx], '||', europarl_fr_clean[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_text(europarl_en_clean, './data/clean/europarl.en')\n",
    "save_text(europarl_fr_clean, './data/clean/europarl.fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# tatoeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfBase = pd.read_csv('./data/raw/tatoeba/sentences.csv', sep='\\t', header=None, names=['label', 'lang', 'text'])\n",
    "dfBase = dfBase.set_index('label')\n",
    "dfLink = pd.read_csv('./data/raw/tatoeba/links.csv', sep='\\t', header=None, names=['ida', 'idb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# en fr sentences label\n",
    "label_en = set(dfBase[dfBase['lang']=='eng'].index.tolist())\n",
    "label_fr = set(dfBase[dfBase['lang']=='fra'].index.tolist())\n",
    "lang_pair = set()\n",
    "# get lang pair label\n",
    "for r in dfLink.itertuples():\n",
    "    if r[1] in label_en and r[2] in label_fr:\n",
    "        lang_pair.add((r[1], r[2]))\n",
    "    elif r[2] in label_en and r[1] in label_fr:\n",
    "        lang_pair.add((r[2], r[1]))\n",
    "lang_enfr = list(lang_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# extract lang pair\n",
    "tatoeba_en = []\n",
    "tatoeba_fr = []\n",
    "for lpair in lang_enfr:\n",
    "    tatoeba_en.append(dfBase.loc[lpair[0],'text'].strip())\n",
    "    tatoeba_fr.append(dfBase.loc[lpair[1],'text'].strip())\n",
    "print('tatoeba has {} sentences pair for english and french'.format(len(tatoeba_en)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sid, win = 50000, 3\n",
    "for idx in range(sid, sid+win):\n",
    "    print(tatoeba_en[idx], '||', tatoeba_fr[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_text(tatoeba_en, './data/clean/tatoeba.en')\n",
    "save_text(tatoeba_fr, './data/clean/tatoeba.fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# jrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('./data/raw/jrc/alignedCorpus-en-fr.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "jrc_en = []\n",
    "jrc_fr = []\n",
    "\n",
    "# extract and clean\n",
    "anti_dup = set()\n",
    "for t in root.findall('.//link'):\n",
    "    \n",
    "    # get text\n",
    "    \n",
    "    # s1 or s2 may contain p\n",
    "    if t.find('s1').find('p') is None:\n",
    "        sent_en = t.find('s1').text\n",
    "    else:\n",
    "        sent_en = ' '.join([p.text for p in t.find('s1').findall('p')])\n",
    "    if t.find('s2').find('p') is None:\n",
    "        sent_fr = t.find('s2').text\n",
    "    else:\n",
    "        sent_fr = ' '.join([p.text for p in t.find('s2').findall('p')])\n",
    "        \n",
    "    # clean\n",
    "    \n",
    "    # start or end space\n",
    "    sent_en = sent_en.strip()\n",
    "    sent_fr = sent_fr.strip()\n",
    "    \n",
    "    # useless line\n",
    "    if sent_en.startswith('Article ') or sent_fr.startswith('Article '):\n",
    "        continue\n",
    "    if ('%gt%' in sent_en) or ('%gt%' in sent_fr):\n",
    "        continue\n",
    "    if ('http://' in sent_en) or ('http://' in sent_fr):\n",
    "        continue\n",
    "    if ('https://' in sent_en) or ('https://' in sent_fr):\n",
    "        continue\n",
    "    if sent_en.startswith('[1]') or sent_fr.startswith('[1]'):\n",
    "        continue\n",
    "    if (sent_en.startswith('(') and sent_en.endswith(')')):\n",
    "        continue\n",
    "    if (sent_fr.startswith('(') and sent_fr.endswith(')')):\n",
    "        continue\n",
    "    if len(sent_en.split('|')) > 3:\n",
    "        continue\n",
    "    if len(sent_fr.split('|')) > 3:\n",
    "        continue\n",
    "    \n",
    "    # mismatch upper percentage\n",
    "    upper_en = sum([1 for c in ''.join(sent_en.split()) if c.isupper()]) * 1.0 / len(''.join(sent_en.split()))\n",
    "    upper_fr = sum([1 for c in ''.join(sent_fr.split()) if c.isupper()]) * 1.0 / len(''.join(sent_fr.split()))\n",
    "    \n",
    "    if abs(upper_en - upper_fr) > 0.5:\n",
    "        continue\n",
    "    \n",
    "    # sentence index\n",
    "    idx_flag = False\n",
    "    for idx in range(1, 6):\n",
    "        pfa = str(idx)+'.'\n",
    "        pfb = str(idx)+' .'\n",
    "        enj = sent_en.startswith(pfa) or sent_en.startswith(pfb)\n",
    "        frj = sent_fr.startswith(pfa) or sent_fr.startswith(pfb)\n",
    "        \n",
    "        if enj and frj:\n",
    "            sent_en = sent_en.split('.', 1)[1].strip()\n",
    "            sent_fr = sent_fr.split('.', 1)[1].strip()\n",
    "            break\n",
    "        \n",
    "        if enj != frj:\n",
    "            idx_flag = True\n",
    "            break\n",
    "    if idx_flag:\n",
    "        continue\n",
    "    \n",
    "    idx_flag = False\n",
    "    for lidx in ['a', 'b', 'c', 'd', 'A', 'B', 'C', 'D']:\n",
    "        \n",
    "        pfa = '('+lidx+')'\n",
    "        pfb = lidx+')'\n",
    "        enj = sent_en.startswith(pfa) or sent_en.startswith(pfb)\n",
    "        frj = sent_fr.startswith(pfa) or sent_fr.startswith(pfb)\n",
    "        \n",
    "        if enj and frj:\n",
    "            sent_en = sent_en.split(')', 1)[1].strip()\n",
    "            sent_fr = sent_fr.split(')', 1)[1].strip()\n",
    "            break\n",
    "        \n",
    "        if enj != frj:\n",
    "            idx_flag = True\n",
    "            break\n",
    "    if idx_flag:\n",
    "        continue\n",
    "    \n",
    "    idx_flag = False\n",
    "    for idx in range(1, 4):\n",
    "        \n",
    "        pfa = '('+'i'*idx+')'\n",
    "        pfb = 'i'*idx+')'\n",
    "        enj = sent_en.startswith(pfa) or sent_en.startswith(pfb)\n",
    "        frj = sent_fr.startswith(pfa) or sent_fr.startswith(pfb)\n",
    "        \n",
    "        if enj and frj:\n",
    "            sent_en = sent_en.split(')', 1)[1].strip()\n",
    "            sent_fr = sent_fr.split(')', 1)[1].strip()\n",
    "            break\n",
    "        \n",
    "        if enj != frj:\n",
    "            idx_flag = True\n",
    "            break\n",
    "    if idx_flag:\n",
    "        continue\n",
    "    \n",
    "    idx_flag = False\n",
    "    for lidx in ['- ']:\n",
    "        \n",
    "        enj = sent_en.startswith(lidx)\n",
    "        frj = sent_fr.startswith(lidx)\n",
    "        \n",
    "        if enj and frj:\n",
    "            sent_en = sent_en[len(lidx):]\n",
    "            sent_fr = sent_fr[len(lidx):]\n",
    "            break\n",
    "        \n",
    "        if enj != frj:\n",
    "            idx_flag = True\n",
    "            break\n",
    "    if idx_flag:\n",
    "        continue\n",
    "    \n",
    "    # replace special\n",
    "    sent_en = sent_en.replace('%quot%', '\"')\n",
    "    sent_fr = sent_fr.replace('%quot%', '\"')\n",
    "    \n",
    "    sent_en = sent_en.replace('ยบ', 'o')\n",
    "    sent_fr = sent_fr.replace('ยบ', 'o')\n",
    "    \n",
    "    \n",
    "    # append\n",
    "    if (sent_en not in anti_dup) and (sent_fr not in anti_dup):\n",
    "        anti_dup.add(sent_en)\n",
    "        anti_dup.add(sent_fr)\n",
    "        \n",
    "        jrc_en.append(sent_en)\n",
    "        jrc_fr.append(sent_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_en, temp_fr = pair_mislength(jrc_en, jrc_fr)\n",
    "temp_en, temp_fr = pair_mislang(temp_en, temp_fr, 'en', 'fr', './model/lid.176.bin', 2)\n",
    "jrc_en_clean, jrc_fr_clean = temp_en, temp_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sid, win = 59000, 3\n",
    "for idx in range(sid, sid+win):\n",
    "    print(jrc_en_clean[idx], '||', jrc_fr_clean[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_text(jrc_en_clean, './data/clean/jrc.en')\n",
    "save_text(jrc_fr_clean, './data/clean/jrc.fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# giga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giga_en = load_text('./data/raw/giga/giga-fren.release2.fixed.en')\n",
    "giga_fr = load_text('./data/raw/giga/giga-fren.release2.fixed.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_en, temp_fr = pair_deduplicate(giga_en, giga_fr)\n",
    "temp_en, temp_fr = pair_mislength(temp_en, temp_fr)\n",
    "temp_en, temp_fr = pair_mislang(temp_en, temp_fr, 'en', 'fr', './model/lid.176.bin', 2)\n",
    "giga_en_clean, giga_fr_clean = temp_en, temp_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
